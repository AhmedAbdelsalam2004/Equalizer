import streamlit as st
import numpy as np
import librosa
import soundfile as sf
import io
import plotly.graph_objects as go
import uuid
import json
import os
import base64
import streamlit.components.v1 as components

# ===================================
# DATA LOADING (PRESETS & FFTS)
# ===================================

@st.cache_resource
def load_presets():
    preset_path = os.path.join(os.path.dirname(__file__), "presets.json")
    try:
        with open(preset_path, "r") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}
    except json.JSONDecodeError:
        return {}

@st.cache_resource
def load_precomputed_ffts():
    """Loads the source FFTs generated by save_source_ffts.py"""
    fft_dir = os.path.join(os.path.dirname(__file__), "saved_ffts")
    if not os.path.exists(fft_dir):
        return None
    try:
        metadata_path = os.path.join(fft_dir, "metadata.npy")
        if not os.path.exists(metadata_path):
            return None

        metadata = np.load(metadata_path, allow_pickle=True).item()
        fft_sources = []
        for i in range(1, metadata["n_sources"] + 1):
            fft = np.load(os.path.join(fft_dir, f"source_{i}_fft.npy"))
            fft_sources.append(fft)

        return {
            "fft_sources": fft_sources,
            "sample_rate": int(metadata["sample_rate"]),
            "max_len": int(metadata["max_len"]),
            "fft_length": int(metadata["fft_length"])
        }
    except Exception as e:
        print(f"Error loading FFTs: {e}")
        return None

PRESETS = load_presets()
PRECOMPUTED_DATA = load_precomputed_ffts()

# Page configuration
st.set_page_config(
    page_title="Signal Equalizer",
    page_icon="üéµ",
    layout="wide"
)

# --- ULTRA-COMPACT CSS STYLING ---
st.markdown("""
<style>
    /* Reduce top padding of the main container */
    .main > div { padding-top: 0.5rem; }
    .block-container { padding-top: 1rem; padding-bottom: 0rem; }
    
    /* Header adjustments */
    .main-header { font-size: 1.5rem; color: #1f77b4; text-align: center; margin-bottom: 0.5rem; margin-top: 0; }
    h1, h2, h3 { margin-top: 0.1rem !important; margin-bottom: 0.1rem !important; }
    
    /* Compact Band Container */
    .band-container { 
        background-color: #f8f9fa; 
        padding: 0.3rem 0.6rem; 
        border-radius: 6px; 
        margin-bottom: 0.2rem; 
        border-left: 3px solid #1f77b4; 
        box-shadow: 0 1px 2px rgba(0,0,0,0.05); 
    }
    
    /* COMPACT SLIDERS */
    .stSlider {
        padding-top: 0rem !important;
        padding-bottom: 0rem !important;
        margin-top: -15px !important;
        margin-bottom: -5px !important;
    }
    
    .stSlider label {
        font-size: 13px !important;
        padding-bottom: 0rem !important;
    }
    
    /* Tighter Widget Spacing */
    div[data-testid="stVerticalBlock"] > div { gap: 0.3rem !important; }
    div[data-testid="column"] { padding: 0rem; }
    
    .stAudio { margin-top: 0rem; margin-bottom: 0.2rem; }
</style>
""", unsafe_allow_html=True)


# ===================================
# COOLEY‚ÄìTUKEY FFT
# ===================================

def next_power_of_two(n):
    return 1 << (n - 1).bit_length()


def bit_reverse_copy(arr):
    N = len(arr)
    if N <= 1:
        return arr.astype(complex)
    bits = N.bit_length() - 1
    rev_indices = np.zeros(N, dtype=int)
    for i in range(N):
        rev_indices[i] = int(format(i, f'0{bits}b')[::-1], 2)
    return arr[rev_indices].astype(complex)


def cooley_tukey_fft(x):
    N = len(x)
    if N == 1:
        return x.astype(complex)
    X = bit_reverse_copy(x)
    size = 2
    while size <= N:
        half = size // 2
        w_step = np.exp(-2j * np.pi / size)
        for i in range(0, N, size):
            w = 1 + 0j
            for j in range(half):
                even = X[i + j]
                odd = X[i + j + half]
                X[i + j] = even + w * odd
                X[i + j + half] = even - w * odd
                w *= w_step
        size *= 2
    return X


def cooley_tukey_ifft(X):
    N = len(X)
    X_conj = np.conj(X)
    fft_of_conj = cooley_tukey_fft(X_conj)
    return (np.conj(fft_of_conj) / N).real


# ===================================
# TRUE AUDIOGRAM CONVERSION
# ===================================

ISO_FREQS = np.array([125, 250, 500, 1000, 2000, 4000, 8000])
ISO_SPL_AT_0_HL = np.array([45.0, 25.5, 11.5, 7.0, 7.0, 8.5, 12.0])


def spl_to_dB_HL(dB_SPL, freqs):
    ref_spl = np.interp(freqs, ISO_FREQS, ISO_SPL_AT_0_HL, left=ISO_SPL_AT_0_HL[0], right=ISO_SPL_AT_0_HL[-1])
    return dB_SPL - ref_spl


def magnitude_to_dB_SPL(magnitude, sample_rate):
    pressure_pa = magnitude
    pressure_pa = np.clip(pressure_pa, 1e-9, None)
    dB_SPL = 20 * np.log10(pressure_pa / 20e-6)
    return dB_SPL


# ===================================
# DYNAMIC PLOTLY PLOTS (COMPACT)
# ===================================

def create_dynamic_fft_plot(fft_data, sample_rate, title, color):
    N = len(fft_data)
    half = N // 2
    magnitude = np.abs(fft_data[:half])
    freqs = np.linspace(0, sample_rate / 2, half)

    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=freqs,
        y=magnitude,
        mode='lines',
        line=dict(color=color, width=1.5),
        name=title,
        fill='tozeroy',
        fillcolor=f"rgba{tuple(int(color.lstrip('#')[i:i+2], 16) for i in (0, 2, 4)) + (0.1,)}" if color.startswith('#') else color
    ))
    fig.update_layout(
        title=dict(text=title, font=dict(size=12)),
        xaxis_title="Frequency (Hz)",
        yaxis_title="Magnitude",
        margin=dict(l=35, r=10, t=30, b=10),
        xaxis=dict(showgrid=True, gridcolor='#eee', rangeslider=dict(visible=False)),
        yaxis=dict(showgrid=True, gridcolor='#eee'),
        plot_bgcolor='rgba(0,0,0,0)',
        hovermode="x unified",
        height=220
    )
    return fig


def create_dynamic_audiogram_plot(fft_data, sample_rate, title, color):
    N = len(fft_data)
    half = N // 2
    magnitude = np.abs(fft_data[:half])
    freqs = np.linspace(0, sample_rate / 2, half)

    valid = freqs >= 100
    freqs = freqs[valid]
    magnitude = magnitude[valid]

    if len(freqs) == 0:
        freqs = np.array([100.0])
        magnitude = np.array([1e-9])

    dB_SPL = magnitude_to_dB_SPL(magnitude, sample_rate)
    dB_HL = spl_to_dB_HL(dB_SPL, freqs)
    dB_HL = np.clip(dB_HL, -10, 120)

    standard_freqs = [125, 250, 500, 1000, 2000, 4000, 8000]
    visible_freqs = [f for f in standard_freqs if f <= sample_rate / 2]

    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=freqs,
        y=dB_HL,
        mode='lines',
        line=dict(color=color, width=2),
        name=title
    ))
    fig.update_layout(
        title=dict(text=title, font=dict(size=12)),
        xaxis_title="Frequency (Hz)",
        yaxis_title="Hearing Level (dB HL)",
        margin=dict(l=35, r=10, t=30, b=10),
        xaxis=dict(
            type="log",
            tickvals=visible_freqs,
            ticktext=[str(f) for f in visible_freqs],
            showgrid=True,
            gridcolor='#eee',
            range=[np.log10(100), np.log10(min(10000, sample_rate / 2))]
        ),
        yaxis=dict(
            showgrid=True,
            gridcolor='#eee',
            range=[120, -10]
        ),
        plot_bgcolor='rgba(0,0,0,0)',
        hovermode="x unified",
        height=220
    )
    return fig


def create_dynamic_spectrogram(spectrogram, sample_rate, n_fft=1024, hop_length=512, title="Spectrogram (dB SPL)"):
    times = np.arange(spectrogram.shape[1]) * hop_length / sample_rate
    freqs = np.linspace(0, sample_rate / 2, spectrogram.shape[0])

    fig = go.Figure(data=go.Heatmap(
        z=spectrogram,
        x=times,
        y=freqs,
        colorscale='Viridis',
        zmin=0,
        zmax=120,
        colorbar=dict(title="dB SPL"),
        hovertemplate="Time: %{x:.2f}s<br>Frequency: %{y:.0f} Hz<br>dB SPL: %{z:.1f}<extra></extra>"
    ))
    fig.update_layout(
        title=dict(text=title, font=dict(size=12)),
        xaxis_title="Time (s)",
        yaxis_title="Frequency (Hz)",
        margin=dict(l=35, r=10, t=30, b=10),
        height=200
    )
    return fig

# ===================================
# CUSTOM SPECTROGRAM (STFT)
# ===================================

def custom_stft(signal, n_fft=1024, hop_length=512):
    window = np.hanning(n_fft)
    pad_len = n_fft - (len(signal) - n_fft) % hop_length
    if pad_len < n_fft:
        pad_len += n_fft
    signal_padded = np.pad(signal, (0, pad_len), mode='constant')
    frames = []
    for i in range(0, len(signal_padded) - n_fft + 1, hop_length):
        frame = signal_padded[i:i + n_fft] * window
        frames.append(frame)
    if not frames:
        frames = [np.pad(signal, (0, n_fft - len(signal)), mode='constant') * window]
    fft_frames = []
    for frame in frames:
        N = next_power_of_two(len(frame))
        frame_padded = np.pad(frame, (0, N - len(frame)), mode='constant')
        fft_result = cooley_tukey_fft(frame_padded)
        fft_frames.append(fft_result[:N // 2])
    return np.abs(np.array(fft_frames).T)


def amplitude_to_dB_SPL(S, sample_rate, n_fft=1024):
    scaling = n_fft / 2
    pressure = S / scaling
    pressure = np.clip(pressure, 1e-9, None)
    dB_SPL = 20 * np.log10(pressure / 20e-6)
    return np.clip(dB_SPL, 0, 120)


# ===================================
# FFT VISUALIZER UTILITY
# ===================================

def prepare_visualizer_data(fft_complex, n_bins=64):
    """
    Reduce FFT complex array to `n_bins` magnitude values scaled to 0-255.
    """
    if fft_complex is None:
        return [0] * n_bins

    mag = np.abs(fft_complex[:len(fft_complex)//2])
    if mag.size == 0:
        return [0] * n_bins

    # split into equal-width bins and average
    edges = np.linspace(0, len(mag), n_bins + 1, dtype=int)
    bins = []
    for i in range(n_bins):
        s, e = edges[i], edges[i+1]
        seg = mag[s:e] if e > s else mag[s:s+1]
        bins.append(float(np.mean(seg)) if seg.size > 0 else 0.0)

    maxv = max(bins) if max(bins) > 0 else 1.0
    scaled = [int(min(255, max(0, (v / maxv) * 255))) for v in bins]
    return scaled


# ===================================
# SIGNAL PROCESSING FUNCTIONS
# ===================================

def apply_gain_mask_to_fft(fft_data, full_freqs, bands, sample_rate):
    N = len(fft_data)
    gain_mask = np.ones(N, dtype=np.float32)
    for band in bands:
        f0 = band["freq"]
        gain = band["gain"]
        bw = band["bandwidth"]
        low = max(0, f0 - bw / 2)
        high = min(sample_rate / 2, f0 + bw / 2)
        in_band = (np.abs(full_freqs) >= low) & (np.abs(full_freqs) <= high)
        gain_mask[in_band] *= gain

    magnitudes = np.abs(fft_data)
    phases = np.angle(fft_data)
    new_magnitudes = magnitudes * gain_mask
    return new_magnitudes * np.exp(1j * phases)


def apply_linear_subtraction(fft_mix, gains, precomputed_data):
    fft_out = fft_mix.copy()
    sources = precomputed_data["fft_sources"]
    n_bins = len(fft_out)

    for i, gain in enumerate(gains):
        if i < len(sources):
            src_fft = sources[i]
            limit = min(n_bins, len(src_fft))
            factor = gain - 1.0

            if abs(factor) > 1e-5:
                fft_out[:limit] += factor * src_fft[:limit]

    return fft_out


def process_output_fft(modified_fft, original_audio_data, sample_rate):
    equalized_full = cooley_tukey_ifft(modified_fft)
    equalized_audio = equalized_full[:len(original_audio_data)]
    st.session_state.equalized_audio = equalized_audio
    st.session_state.eq_applied = True

    N_orig = len(equalized_audio)
    N_pad = next_power_of_two(N_orig)
    eq_padded = np.pad(equalized_audio, (0, N_pad - N_orig),
                       mode='constant') if N_pad > N_orig else equalized_audio
    eq_fft = cooley_tukey_fft(eq_padded)

    st.session_state.output_fft_linear = create_dynamic_fft_plot(eq_fft, sample_rate, "Output FFT", 'orange')
    st.session_state.output_audiogram = create_dynamic_audiogram_plot(eq_fft, sample_rate,
                                                             "Output Audiogram", 'orange')

    S_output = custom_stft(equalized_audio, n_fft=1024, hop_length=512)
    S_output_dB = amplitude_to_dB_SPL(S_output, sample_rate, n_fft=1024)
    st.session_state.output_spectrogram = create_dynamic_spectrogram(S_output_dB, sample_rate, 1024, 512,
                                                            "Output Spectrogram")

    return equalized_audio


# ===================================
# Session State
# ===================================

if 'initialized' not in st.session_state:
    st.session_state.audio_data = None
    st.session_state.sample_rate = None
    st.session_state.equalized_audio = None
    st.session_state.fft_data = None
    st.session_state.full_freqs = None
    st.session_state.eq_bands = []
    st.session_state.last_audio_hash = None
    st.session_state.input_fft_linear = None
    st.session_state.input_audiogram = None
    st.session_state.input_spectrogram = None
    st.session_state.output_fft_linear = None
    st.session_state.output_audiogram = None
    st.session_state.output_spectrogram = None
    st.session_state.eq_applied = False
    st.session_state.initialized = True 

# ===================================
# Sidebar
# ===================================

st.markdown('<h1 class="main-header">üéµ Signal Equalizer</h1>', unsafe_allow_html=True)

with st.sidebar:
    st.subheader("‚öôÔ∏è Configuration")

    freq_scale = st.radio("Display", ["Linear FFT", "Audiogram (dB HL)"], index=0)

    mode_options = ["Generic Mode", "Musical Instruments Mode", "Animal Sounds Mode", "Human Voices Mode"]
    mode_icons = {
        "Generic Mode": "üõ†Ô∏è Generic",
        "Musical Instruments Mode": "üéª Musical Instruments",
        "Animal Sounds Mode": "ü¶Å Animal Sounds",
        "Human Voices Mode": "üó£Ô∏è Human Voices"
    }

    mode = st.selectbox(
        "Mode:",
        mode_options,
        format_func=lambda x: mode_icons.get(x, x)
    )

    uploaded_file = st.file_uploader("üìÇ Upload Audio", type=['wav', 'mp3', 'flac'])

# ===================================
# Load & Preprocess (ONCE ONLY)
# ===================================

if uploaded_file is not None:
    file_hash = hash(uploaded_file.getvalue())
    if file_hash != st.session_state.last_audio_hash:
        with io.BytesIO(uploaded_file.getvalue()) as buffer:
            audio_data, sample_rate = librosa.load(buffer, sr=None)

        N_orig = len(audio_data)
        N_pad = next_power_of_two(N_orig)
        audio_padded = np.pad(audio_data, (0, N_pad - N_orig), mode='constant') if N_pad > N_orig else audio_data

        with st.spinner("‚è≥ Analyzing..."):
            try:
                fft_data = cooley_tukey_fft(audio_padded)
                full_freqs = np.fft.fftfreq(len(fft_data), d=1 / sample_rate)

                input_fft_linear = create_dynamic_fft_plot(fft_data, sample_rate, "Input FFT", 'steelblue')
                input_audiogram = create_dynamic_audiogram_plot(fft_data, sample_rate, "Input Audiogram",
                                                                'steelblue')

                S_input = custom_stft(audio_data, n_fft=1024, hop_length=512)
                S_input_dB = amplitude_to_dB_SPL(S_input, sample_rate, n_fft=1024)
                input_spectrogram = create_dynamic_spectrogram(S_input_dB, sample_rate, 1024, 512, "Input Spectrogram")

                st.session_state.audio_data = audio_data
                st.session_state.sample_rate = sample_rate
                st.session_state.fft_data = fft_data
                st.session_state.full_freqs = full_freqs
                st.session_state.equalized_audio = audio_data.copy()
                st.session_state.last_audio_hash = file_hash
                st.session_state.eq_bands = []
                st.session_state.input_fft_linear = input_fft_linear
                st.session_state.input_audiogram = input_audiogram
                st.session_state.input_spectrogram = input_spectrogram
                st.session_state.output_fft_linear = None
                st.session_state.output_audiogram = None
                st.session_state.output_spectrogram = None
                st.session_state.eq_applied = False
            except Exception as e:
                st.error(f"Error: {e}")


# ===================================
# AUDIO PLAYER/VISUALIZER COMPONENT
# ===================================

def create_audio_visualizer(audio_b64, player_id, sample_rate):
    """Create an audio player with real-time FFT visualization"""
    html_code = f"""
<!DOCTYPE html>
<html>
<head>
<style>
body {{ margin: 0; padding: 0; font-family: Inter, system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial; }}
.audio-player {{
  background: linear-gradient(135deg, #0f1724 0%, #1a2332 100%);
  color: #e6eef8;
  border: 1px solid #2563eb;
  padding: 16px;
  border-radius: 12px;
  box-shadow: 0 4px 6px rgba(0,0,0,0.3);
}}
.canvas-wrap {{ 
  margin-bottom: 12px;
  position: relative; 
  background: #0a0f1a;
  border-radius: 8px;
  overflow: hidden;
  border: 1px solid #1e3a5f;
  width: 100%;
}}
canvas {{ display: block; width: 100%; height: 120px; background: #0a0f1a; }}
.controls {{ 
  display: flex; 
  gap: 10px; 
  align-items: center; 
  justify-content: space-between;
}}
.control-buttons {{ display: flex; gap: 8px; }}
.btn {{
  background: linear-gradient(135deg, #2563eb 0%, #1d4ed8 100%);
  color: white; 
  border: none; 
  padding: 10px 16px; 
  border-radius: 8px; 
  cursor: pointer;
  font-size: 14px;
  font-weight: 600;
}}
.btn:hover {{ 
  background: linear-gradient(135deg, #1d4ed8 0%, #1e40af 100%);
}}
.btn:active {{ transform: scale(0.98); }}
.time-info {{ 
  font-size: 13px; 
  color: #cbd5e1;
}}
.status {{ 
  position: absolute;
  top: 8px;
  right: 8px;
  font-size: 11px;
  color: #38bdf8;
  background: rgba(0,0,0,0.5);
  padding: 4px 8px;
  border-radius: 4px;
}}
</style>
</head>
<body>
<div class="audio-player">
  <div class="canvas-wrap">
    <canvas id="canvas-{player_id}"></canvas>
    <div class="status" id="status-{player_id}">Ready</div>
  </div>
  
  <div class="controls">
    <div class="control-buttons">
      <button class="btn" id="play-btn-{player_id}">‚ñ∂ Play</button>
      <button class="btn" id="stop-btn-{player_id}">‚èπ Stop</button>
    </div>
    <div class="time-info" id="time-{player_id}">0:00 / 0:00</div>
  </div>
</div>

<script>
(function() {{
  const canvas = document.getElementById('canvas-{player_id}');
  const ctx = canvas.getContext('2d');
  const playBtn = document.getElementById('play-btn-{player_id}');
  const stopBtn = document.getElementById('stop-btn-{player_id}');
  const timeDisplay = document.getElementById('time-{player_id}');
  const statusDisplay = document.getElementById('status-{player_id}');
  
  let audioContext;
  let analyser;
  let audioBuffer;
  let source;
  let isPlaying = false;
  let animationId = null;
  let startTime = 0;
  let pauseTime = 0;
  const audioData = 'data:audio/wav;base64,{audio_b64}';
  
  function formatTime(seconds) {{
    const mins = Math.floor(seconds / 60);
    const secs = Math.floor(seconds % 60);
    return mins + ':' + (secs < 10 ? '0' : '') + secs;
  }}
  
  function updateTimeDisplay() {{
    if (!audioBuffer) return;
    const duration = audioBuffer.duration;
    const current = isPlaying ? Math.min(audioContext.currentTime - startTime + pauseTime, duration) : pauseTime;
    timeDisplay.textContent = formatTime(current) + ' / ' + formatTime(duration);
  }}
  
  function resizeCanvas() {{
    const rect = canvas.getBoundingClientRect();
    canvas.width = rect.width;
    canvas.height = 120;
  }}
  
  function drawVisualization() {{
    if (!analyser) return;
    
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    analyser.getByteFrequencyData(dataArray);
    
    const width = canvas.width;
    const height = canvas.height;
    
    ctx.fillStyle = '#0a0f1a';
    ctx.fillRect(0, 0, width, height);
    
    const barWidth = width / bufferLength;
    const gradient = ctx.createLinearGradient(0, height, width, 0);
    gradient.addColorStop(0, '#38bdf8');
    gradient.addColorStop(0.5, '#3b82f6');
    gradient.addColorStop(1, '#8b5cf6');
    
    for (let i = 0; i < bufferLength; i++) {{
      const barHeight = (dataArray[i] / 255) * height * 0.95;
      const x = i * barWidth;
      const y = height - barHeight;
      
      ctx.fillStyle = gradient;
      ctx.fillRect(x, y, barWidth - 1, barHeight);
      
      if (barHeight > 3) {{
        ctx.fillStyle = 'rgba(255, 255, 255, 0.6)';
        ctx.fillRect(x, y, barWidth - 1, 2);
      }}
    }}
    
    updateTimeDisplay();
    
    if (isPlaying) {{
      animationId = requestAnimationFrame(drawVisualization);
    }}
  }}
  
  async function initAudio() {{
    if (audioContext) return;
    
    try {{
      const response = await fetch(audioData);
      const arrayBuffer = await response.arrayBuffer();
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      analyser.smoothingTimeConstant = 0.8;
      analyser.connect(audioContext.destination);
      statusDisplay.textContent = 'Ready';
      updateTimeDisplay();
    }} catch (error) {{
      console.error('Audio init error:', error);
      statusDisplay.textContent = 'Error';
    }}
  }}
  
  async function play() {{
    if (!audioContext) await initAudio();
    if (isPlaying) return;
    
    source = audioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(analyser);
    
    startTime = audioContext.currentTime;
    source.start(0, pauseTime);
    
    source.onended = () => {{
      isPlaying = false;
      pauseTime = 0;
      playBtn.textContent = '‚ñ∂ Play';
      statusDisplay.textContent = 'Finished';
      if (animationId) cancelAnimationFrame(animationId);
      ctx.clearRect(0, 0, canvas.width, canvas.height);
    }};
    
    isPlaying = true;
    playBtn.textContent = '‚è∏ Pause';
    statusDisplay.textContent = 'Playing';
    drawVisualization();
  }}
  
  function pause() {{
    if (!isPlaying) return;
    source.stop();
    pauseTime += audioContext.currentTime - startTime;
    isPlaying = false;
    playBtn.textContent = '‚ñ∂ Play';
    statusDisplay.textContent = 'Paused';
    if (animationId) cancelAnimationFrame(animationId);
  }}
  
  function stop() {{
    if (source) source.stop();
    isPlaying = false;
    pauseTime = 0;
    playBtn.textContent = '‚ñ∂ Play';
    statusDisplay.textContent = 'Stopped';
    if (animationId) cancelAnimationFrame(animationId);
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    updateTimeDisplay();
  }}
  
  playBtn.onclick = async () => {{
    if (isPlaying) pause();
    else await play();
  }};
  
  stopBtn.onclick = stop;
  
  window.addEventListener('load', () => {{
    resizeCanvas();
    initAudio();
  }});
  
  window.addEventListener('resize', () => {{
    resizeCanvas();
  }});
}})();
</script>
</body>
</html>
"""
    components.html(html_code, height=240, scrolling=False)


# ===================================
# Main Layout - LAYOUT SETUP
# ===================================

if st.session_state.audio_data is not None:
    col1, col2 = st.columns([1, 1])

    # STEP 2: Render Input Signal with Audio Visualizer
    with col1:
        st.markdown("#### üìä Input")
        st.success(f"‚úÖ {uploaded_file.name}")

        # Convert input audio to base64 for HTML audio element
        input_bytes = io.BytesIO()
        sf.write(input_bytes, st.session_state.audio_data, st.session_state.sample_rate, format='WAV')
        input_bytes.seek(0)
        input_audio_b64 = base64.b64encode(input_bytes.read()).decode()
        
        # Create audio player/visualizer for input
        create_audio_visualizer(input_audio_b64, "input", st.session_state.sample_rate)

        if freq_scale == "Linear FFT":
            st.plotly_chart(st.session_state.input_fft_linear, use_container_width=True)
        else:
            st.plotly_chart(st.session_state.input_audiogram, use_container_width=True)


# ===================================
# Equalizer Controls
# ===================================

if st.session_state.audio_data is not None:
    st.markdown(f"#### üéõÔ∏è {mode}")

    if mode == "Generic Mode":
        if st.button("‚ûï Add Band"):
            st.session_state.eq_bands.append({
                "id": str(uuid.uuid4()),
                "freq": 1000,
                "gain": 1.0,
                "bandwidth": 200
            })
            st.rerun()

        for idx, band in enumerate(st.session_state.eq_bands, start=1):
            with st.container():
                st.markdown('<div class="band-container">', unsafe_allow_html=True)
                cols = st.columns([3, 3, 3, 1])
                freq = cols[0].slider(f"Freq (Hz)", 20, st.session_state.sample_rate // 2, int(band["freq"]), key=f"f_{band['id']}")
                gain = cols[1].slider(f"Gain", 0.0, 2.0, float(band["gain"]), key=f"g_{band['id']}")
                bw = cols[2].slider(f"Width", 10, 5000, int(band["bandwidth"]), key=f"b_{band['id']}")
                if cols[3].button("üóëÔ∏è", key=f"d_{band['id']}"):
                    st.session_state.eq_bands = [b for b in st.session_state.eq_bands if b["id"] != band["id"]]
                    st.rerun()
                band.update({"freq": freq, "gain": gain, "bandwidth": bw})
                st.markdown('</div>', unsafe_allow_html=True)

        if st.session_state.fft_data is not None and st.session_state.eq_bands:
            modified_fft = apply_gain_mask_to_fft(
                st.session_state.fft_data,
                st.session_state.full_freqs,
                st.session_state.eq_bands,
                st.session_state.sample_rate
            )
            process_output_fft(modified_fft, st.session_state.audio_data, st.session_state.sample_rate)


    else:
        # CUSTOM MODES
        if mode not in PRESETS:
            st.warning(f"No preset for '{mode}'")
        else:
            sources = PRESETS[mode]
            cols = st.columns(4) 
            source_gains = []

            for i, source_name in enumerate(sources):
                with cols[i % 4]:
                    gain = st.slider(
                        source_name.capitalize(),
                        0.0, 2.0, 1.0,
                        key=f"preset_{mode}_{source_name}"
                    )
                    source_gains.append(gain)

            if st.session_state.fft_data is not None:
                if PRECOMPUTED_DATA is not None:
                    modified_fft = apply_linear_subtraction(
                        st.session_state.fft_data,
                        source_gains,
                        PRECOMPUTED_DATA
                    )
                else:
                    preset_bands = []
                    g_idx = 0
                    for s_name in sources:
                        g_val = source_gains[g_idx]
                        g_idx += 1
                        for center, bw in sources[s_name]:
                            preset_bands.append({"freq": center, "gain": g_val, "bandwidth": bw})

                    modified_fft = apply_gain_mask_to_fft(
                        st.session_state.fft_data,
                        st.session_state.full_freqs,
                        preset_bands,
                        st.session_state.sample_rate
                    )

                process_output_fft(modified_fft, st.session_state.audio_data, st.session_state.sample_rate)


else:
    st.info("üëÜ Upload an audio file")


# ===================================
# Main Layout - OUTPUT DISPLAY
# ===================================

if st.session_state.audio_data is not None and st.session_state.eq_applied:
    with col2:
        st.markdown("#### üìä Output")
        
        # Convert equalized audio to base64 for HTML audio element
        equalized_bytes = io.BytesIO()
        sf.write(equalized_bytes, st.session_state.equalized_audio, st.session_state.sample_rate, format='WAV')
        equalized_bytes.seek(0)
        output_audio_b64 = base64.b64encode(equalized_bytes.read()).decode()
        
        # Create audio player/visualizer for output
        create_audio_visualizer(output_audio_b64, "output", st.session_state.sample_rate)

        if freq_scale == "Linear FFT":
            st.plotly_chart(st.session_state.output_fft_linear, use_container_width=True)
        else:
            st.plotly_chart(st.session_state.output_audiogram, use_container_width=True)

# ===================================
# Spectrograms (if space available)
# ===================================

if st.session_state.audio_data is not None:
    st.markdown("---")
    spec_col1, spec_col2 = st.columns([1, 1])
    
    with spec_col1:
        if st.session_state.input_spectrogram is not None:
            st.plotly_chart(st.session_state.input_spectrogram, use_container_width=True)
    
    with spec_col2:
        if st.session_state.eq_applied and st.session_state.output_spectrogram is not None:
            st.plotly_chart(st.session_state.output_spectrogram, use_container_width=True)
